<!doctype html><html><head><link rel=stylesheet href=https://devlog.hexops.com/assets/font/stylesheet.css><link rel=stylesheet href=https://devlog.hexops.com/main.959becff4f3640c2ba19521949a93832fa147f951930301c1c72e74e25823239.css><script async defer data-domain=hexops.com src=https://hexops.com/opendata.js></script><meta charset=utf-8><title>Unicode data file compression: achieving 40-70% reduction over gzip alone | Hexops' devlog</title><link rel=canonical href=https://devlog.hexops.com/2021/unicode-data-file-compression/><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="Unicode data file compression: achieving 40-70% reduction over gzip alone"><meta property="og:description" content="A little story about how writing a domain-specific compression algorithm in a few days can sometimes yield big benefits, why it's sometimes worth giving it a shot, and how to tell when you should try. Note: this is about Unicode spec data files, not general purpose text compression."><meta property="og:type" content="article"><meta property="og:url" content="https://devlog.hexops.com/2021/unicode-data-file-compression/"><meta property="article:section" content="2021"><meta property="article:published_time" content="2021-07-03T00:00:00+00:00"><meta property="article:modified_time" content="2021-07-03T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Unicode data file compression: achieving 40-70% reduction over gzip alone"><meta name=twitter:description content="A little story about how writing a domain-specific compression algorithm in a few days can sometimes yield big benefits, why it's sometimes worth giving it a shot, and how to tell when you should try. Note: this is about Unicode spec data files, not general purpose text compression."></head><body><div class=navbar><div class=content><a href=/ class=logo><img src=https://raw.githubusercontent.com/hexops/media/234e15f265b19743c580a078b2d68660c92675d4/logo.svg>' devlog</a><div><a href=https://hexops.com/about class=item>About</a>
<a href=/archives class=item>Archives</a>
<a href=https://github.com/hexops class=item>GitHub</a></div></div></div><div id=content><main aria-role=main class=main-single><div class=single><header class=header><h1>Unicode data file compression: achieving 40-70% reduction over gzip alone</h1><div class=metadata><time>July 3, 2021</time>
• <a class=category href=/categories/zig>zig</a>
• <a class=category href=/categories/unicode>unicode</a>
• <a class=category href=/categories/compression>compression</a></div></header><p>A little story about how writing a domain-specific compression algorithm in a few days can sometimes yield big benefits, why it&rsquo;s sometimes worth giving it a shot, and how to tell when you should try. Note: this is about Unicode spec data files, not general purpose text compression.</p><ul><li><a href=#background>Background</a></li><li><a href=#problem>Problem</a></li><li><a href=#investigation>Investigation</a><ul><li><a href=#binary-encoding>Binary encoding?</a></li><li><a href=#differential-encodingcompression>Differential encoding/compression?</a></li><li><a href=#go-implementation>Go implementation</a></li></ul></li><li><a href=#zig-implementation>Zig implementation</a><ul><li><a href=#differential-encoding-state-machine>Differential encoding state machine</a></li><li><a href=#a-stream-of-op-codes>A stream of op codes</a></li><li><a href=#iteratively-finding-the-most-lucrative-opcodes>Iteratively finding the most lucrative opcodes</a></li><li><a href=#a-stream-of-opcodes-for-a-state-machine-a-natural-progression-from-a-binary-format>A stream of opcodes for a state machine: a natural progression from a binary format?</a></li></ul></li><li><a href=#results-better-than-gzipbrotli-and-even-better-with-them>Results? Better than gzip/brotli; and even better <em>with</em> them!</a><ul><li><a href=#why-test-with-gzipbrotli-but-not-others>Why test with gzip/brotli but not others?</a></li><li><a href=#how-complex-is-the-implementation>How complex is the implementation?</a></li></ul></li><li><a href=#notable-mention>Notable mention</a></li><li><a href=#conclusion>Conclusion</a></li></ul><h2 id=background>Background</h2><p>Two weeks ago, I began using <a href=https://github.com/jecolon/ziglyph>Ziglyph</a> (&ldquo;Unicode processing with Zig, and a UTF-8 string type: Zigstr.") - an awesome library by <a href=https://github.com/jecolon>@jecolon</a>, for grapheme cluster sorting in <a href=https://github.com/hexops/zorex>Zorex, an omnipotent regexp engine</a>.</p><p>I don&rsquo;t personally have any prior experience working with the lower level details of Unicode, or compression algorithms for that matter.</p><h2 id=problem>Problem</h2><p>As I stumbled into the wondrous world that is Unicode text sorting (see also my article: <a href=2021-06-27-unicode-sorting-why-browsers-added-special-emoji-matching.md>Unicode sorting is hard & why browsers added special emoji matching to regexp</a>) and began using Ziglyph, I came across an issue: the standard Unicode collation algorithm, which Ziglyph implements, depends on some large Unicode data tables for normalization and sort keys - even gzipped these were fairly large:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>hexops-mac:zorex slimsag$ du -sh asset/*
308K	asset/uca-allkeys.txt.gz
260K	asset/ucd-UnicodeData.txt.gz
</code></pre></div><p>These file sizes may seem small, but one of my goals is to make Zorex a real competitor to e.g. a browser&rsquo;s native regexp engine. That&rsquo;s challenging because WebAssembly bundle sizes matter <em>a lot</em> in that context, and using the browser&rsquo;s regexp implementation is virtually free.</p><h2 id=investigation>Investigation</h2><p>I set out to try and reduce the size of these data files. First I <a href=https://github.com/jecolon/ziglyph/issues/3>opened an issue and asked</a> if anyone else had thoughts around reducing the size of this data. The author of Ziglyph <a href=https://github.com/jecolon>@jecolon</a> is awesome and readily had some ideas and was able to reduce the two files substantially by removing unnecessary data (such as comments, etc.)</p><p>Curious how much further we could go, I kept squinting at the data files (warning: large):</p><ul><li><a href=http://www.unicode.org/Public/UCA/latest/allkeys.txt>http://www.unicode.org/Public/UCA/latest/allkeys.txt</a></li><li><a href=http://www.unicode.org/Public/UNIDATA/UnicodeData.txt>http://www.unicode.org/Public/UNIDATA/UnicodeData.txt</a></li></ul><h3 id=binary-encoding>Binary encoding?</h3><p>My first thoughts were that a binary encoding would likely reduce the size a lot. I pulled in some help from Hobbyist reverse engineer <a href=https://github.com/Andoryuuta>@Andoryuuta</a> and he got started on a binary encoding for UnicodeData.txt based on the spec. With that, he was able to reduce the original 1.9M allkeys.txt file down to 250K (125K gzipped) - quite a win.</p><h3 id=differential-encodingcompression>Differential encoding/compression?</h3><p>My secondary thought was that, scrolling through these data files it was obvious most entries were derived from prior entries. Many entries were long runs of data where the next entry had the same value, plus a small increment. For example, at the start of the <code>allkeys.txt</code> file:</p><pre><code>0000  ; [.0000.0000.0000] # NULL (in ISO 6429)
0001  ; [.0000.0000.0000] # START OF HEADING (in ISO 6429)
0002  ; [.0000.0000.0000] # START OF TEXT (in ISO 6429)
0003  ; [.0000.0000.0000] # END OF TEXT (in ISO 6429)
0004  ; [.0000.0000.0000] # END OF TRANSMISSION (in ISO 6429)
0005  ; [.0000.0000.0000] # ENQUIRY (in ISO 6429)
0006  ; [.0000.0000.0000] # ACKNOWLEDGE (in ISO 6429)
0007  ; [.0000.0000.0000] # BELL (in ISO 6429)
0008  ; [.0000.0000.0000] # BACKSPACE (in ISO 6429)
000E  ; [.0000.0000.0000] # SHIFT OUT (in ISO 6429)
000F  ; [.0000.0000.0000] # SHIFT IN (in ISO 6429)
</code></pre><p>Of course, not all sections are so sequential. Many sections are a bit more arbitrary:</p><pre><code>FF9A  ; [.4304.0020.0012] # HALFWIDTH KATAKANA LETTER RE
32F9  ; [.4304.0020.0013] # CIRCLED KATAKANA RE
3355  ; [.4304.0020.001C][.42FB.0020.001C] # SQUARE REMU
3356  ; [.4304.0020.001C][.430A.0020.001C][.42EE.0020.001C][.42E3.0020.001C][.0000.0037.001C][.430A.0020.001C] # SQUARE RENTOGEN
308D  ; [.4305.0020.000E] # HIRAGANA LETTER RO
31FF  ; [.4305.0020.000F] # KATAKANA LETTER SMALL RO
30ED  ; [.4305.0020.0011] # KATAKANA LETTER RO
</code></pre><p>Still, there are obvious patterns one can see in the way these values change.</p><h3 id=go-implementation>Go implementation</h3><p>I did a quick hacky Go implementation of differential encoding on these files to see how well that would work. The results were pretty good, and already beat just <code>gzip -9</code> compression of the files:</p><table><thead><tr><th>File</th><th>Original</th><th>Original + <code>gzip -9</code></th><th>My compression</th><th>My compression + <code>gzip -9</code></th></tr></thead><tbody><tr><td>Decompositions.txt</td><td>72K</td><td>28K</td><td>48K</td><td>12K</td></tr><tr><td>allkeys-minimal.txt</td><td>500K</td><td>148K</td><td>204K</td><td>36K</td></tr></tbody></table><p>However, because I chose to do these experiments in Go I found a number of inefficiencies:</p><ul><li>There were a lot of locations where I encoded things as 8-bit unsigned integers (Go&rsquo;s smallest value type) instead of a more optimal 4-bit unsigned integer. I could&rsquo;ve done bit shifting, but it would&rsquo;ve been annoying.</li><li>There were also many places where I encoded Unicode codepoints as 32-bit unsigned integers, rather than a more optimal 21-bit unsigned integer (because valid Unicode codepoints do not exceed that range.)</li></ul><p>For a real implementation, I switched over to Zig.</p><h2 id=zig-implementation>Zig implementation</h2><p>Actually, two things made working on this in Zig much easier than in Go:</p><ol><li>Zig has variable bit-width integers: I could just write <code>u4</code> and <code>u21</code> values instead of needing to handle bit packing within larger size integers myself. That was <em>nice</em>.</li><li>In the Zig standard library it provides:</li></ol><ul><li><a href=https://sourcegraph.com/github.com/ziglang/zig@0.8.0/-/blob/lib/std/io/bit_writer.zig?L152-202><code>std.io.BitWriter</code></a></li><li><a href=https://sourcegraph.com/github.com/ziglang/zig@0.8.0/-/blob/lib/std/io/bit_reader.zig?L176-248><code>std.io.BitReader</code></a></li></ul><p>With these two features, it became incredibly easy to write the most optimal bit-packed encoding of the data.</p><p>In fact, the basic uncompressed binary format <a href=https://github.com/jecolon/ziglyph/pull/7/commits/7d4042d8df21cc11eaf42177c2f4d9b3afd9c4a7>was only a few lines to encode</a>:</p><div class=highlight><pre class=chroma><code class=language-zig data-lang=zig><span class=kr>pub</span><span class=w> </span><span class=k>fn</span><span class=w> </span><span class=n>compressTo</span><span class=p>(</span><span class=n>self</span><span class=o>:</span><span class=w> </span><span class=o>*</span><span class=n>DecompFile</span><span class=p>,</span><span class=w> </span><span class=n>writer</span><span class=o>:</span><span class=w> </span><span class=n>anytype</span><span class=p>)</span><span class=w> </span><span class=o>!</span><span class=kt>void</span><span class=w> </span><span class=p>{</span><span class=w>
</span><span class=w>    </span><span class=kr>var</span><span class=w> </span><span class=n>buf_writer</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>std</span><span class=p>.</span><span class=n>io</span><span class=p>.</span><span class=n>bufferedWriter</span><span class=p>(</span><span class=n>writer</span><span class=p>);</span><span class=w>
</span><span class=w>    </span><span class=kr>var</span><span class=w> </span><span class=n>out</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>std</span><span class=p>.</span><span class=n>io</span><span class=p>.</span><span class=n>bitWriter</span><span class=p>(.</span><span class=n>Little</span><span class=p>,</span><span class=w> </span><span class=n>buf_writer</span><span class=p>.</span><span class=n>writer</span><span class=p>());</span><span class=w>
</span><span class=w>    </span><span class=k>try</span><span class=w> </span><span class=n>out</span><span class=p>.</span><span class=n>writeBits</span><span class=p>(</span><span class=nb>@intCast</span><span class=p>(</span><span class=kt>u16</span><span class=p>,</span><span class=w> </span><span class=n>self</span><span class=p>.</span><span class=n>entries</span><span class=p>.</span><span class=n>items</span><span class=p>.</span><span class=n>len</span><span class=p>),</span><span class=w> </span><span class=mi>16</span><span class=p>);</span><span class=w>
</span><span class=w>    </span><span class=k>while</span><span class=w> </span><span class=p>(</span><span class=n>self</span><span class=p>.</span><span class=n>next</span><span class=p>())</span><span class=w> </span><span class=o>|</span><span class=n>entry</span><span class=o>|</span><span class=w> </span><span class=p>{</span><span class=w>
</span><span class=w>        </span><span class=k>try</span><span class=w> </span><span class=n>out</span><span class=p>.</span><span class=n>writeBits</span><span class=p>(</span><span class=n>entry</span><span class=p>.</span><span class=n>key_len</span><span class=p>,</span><span class=w> </span><span class=mi>3</span><span class=p>);</span><span class=w>
</span><span class=w>        </span><span class=n>_</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>try</span><span class=w> </span><span class=n>out</span><span class=p>.</span><span class=n>write</span><span class=p>(</span><span class=n>entry</span><span class=p>.</span><span class=n>key</span><span class=p>[</span><span class=mi>0</span><span class=p>..</span><span class=n>entry</span><span class=p>.</span><span class=n>key_len</span><span class=p>]);</span><span class=w>
</span><span class=w>        </span><span class=k>try</span><span class=w> </span><span class=n>out</span><span class=p>.</span><span class=n>writeBits</span><span class=p>(</span><span class=nb>@enumToInt</span><span class=p>(</span><span class=n>entry</span><span class=p>.</span><span class=n>value</span><span class=p>.</span><span class=n>form</span><span class=p>),</span><span class=w> </span><span class=nb>@bitSizeOf</span><span class=p>(</span><span class=n>Form</span><span class=p>));</span><span class=w>
</span><span class=w>        </span><span class=k>try</span><span class=w> </span><span class=n>out</span><span class=p>.</span><span class=n>writeBits</span><span class=p>(</span><span class=n>entry</span><span class=p>.</span><span class=n>value</span><span class=p>.</span><span class=n>len</span><span class=p>,</span><span class=w> </span><span class=mi>5</span><span class=p>);</span><span class=w>
</span><span class=w>        </span><span class=k>for</span><span class=w> </span><span class=p>(</span><span class=n>entry</span><span class=p>.</span><span class=n>value</span><span class=p>.</span><span class=n>seq</span><span class=p>[</span><span class=mi>0</span><span class=p>..</span><span class=n>entry</span><span class=p>.</span><span class=n>value</span><span class=p>.</span><span class=n>len</span><span class=p>])</span><span class=w> </span><span class=o>|</span><span class=n>s</span><span class=o>|</span><span class=w> </span><span class=k>try</span><span class=w> </span><span class=n>out</span><span class=p>.</span><span class=n>writeBits</span><span class=p>(</span><span class=n>s</span><span class=p>,</span><span class=w> </span><span class=mi>21</span><span class=p>);</span><span class=w>
</span><span class=w>    </span><span class=p>}</span><span class=w>
</span><span class=w>    </span><span class=k>try</span><span class=w> </span><span class=n>out</span><span class=p>.</span><span class=n>flushBits</span><span class=p>();</span><span class=w>
</span><span class=w>    </span><span class=k>try</span><span class=w> </span><span class=n>buf_writer</span><span class=p>.</span><span class=n>flush</span><span class=p>();</span><span class=w>
</span><span class=w></span><span class=p>}</span><span class=w>
</span></code></pre></div><h3 id=differential-encoding-state-machine>Differential encoding state machine</h3><p>To handle the compression, I started out <em>really</em> simple. First I encoded just a binary version of the data with no compression. The most important thing was to get to a point where I could start testing some theories about what would compress the data really well, and validate that it was in fact being losslessly compressed/decompressed without issues via tests:</p><div class=highlight><pre class=chroma><code class=language-zig data-lang=zig><span class=k>test</span><span class=w> </span><span class=s>&#34;compression_is_lossless&#34;</span><span class=w> </span><span class=p>{</span><span class=w>
</span><span class=w>    </span><span class=kr>const</span><span class=w> </span><span class=n>allocator</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>testing</span><span class=p>.</span><span class=n>allocator</span><span class=p>;</span><span class=w>
</span><span class=w>
</span><span class=w>    </span><span class=c1>// Compress UnicodeData.txt -&gt; Decompositions.bin
</span><span class=c1></span><span class=w>    </span><span class=kr>var</span><span class=w> </span><span class=n>file</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>try</span><span class=w> </span><span class=n>parseFile</span><span class=p>(</span><span class=n>allocator</span><span class=p>,</span><span class=w> </span><span class=s>&#34;src/data/ucd/UnicodeData.txt&#34;</span><span class=p>);</span><span class=w>
</span><span class=w>    </span><span class=k>defer</span><span class=w> </span><span class=n>file</span><span class=p>.</span><span class=n>deinit</span><span class=p>();</span><span class=w>
</span><span class=w>    </span><span class=k>try</span><span class=w> </span><span class=n>file</span><span class=p>.</span><span class=n>compressToFile</span><span class=p>(</span><span class=s>&#34;src/data/ucd/Decompositions.bin&#34;</span><span class=p>);</span><span class=w>
</span><span class=w>
</span><span class=w>    </span><span class=c1>// Reset the raw file iterator.
</span><span class=c1></span><span class=w>    </span><span class=n>file</span><span class=p>.</span><span class=n>iter</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=mi>0</span><span class=p>;</span><span class=w>
</span><span class=w>
</span><span class=w>    </span><span class=c1>// Decompress the file.
</span><span class=c1></span><span class=w>    </span><span class=kr>var</span><span class=w> </span><span class=n>decompressed</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>try</span><span class=w> </span><span class=n>decompressFile</span><span class=p>(</span><span class=n>allocator</span><span class=p>,</span><span class=w> </span><span class=s>&#34;src/data/ucd/Decompositions.bin&#34;</span><span class=p>);</span><span class=w>
</span><span class=w>    </span><span class=k>defer</span><span class=w> </span><span class=n>decompressed</span><span class=p>.</span><span class=n>deinit</span><span class=p>();</span><span class=w>
</span><span class=w>    </span><span class=k>while</span><span class=w> </span><span class=p>(</span><span class=n>file</span><span class=p>.</span><span class=n>next</span><span class=p>())</span><span class=w> </span><span class=o>|</span><span class=n>expected</span><span class=o>|</span><span class=w> </span><span class=p>{</span><span class=w>
</span><span class=w>        </span><span class=kr>var</span><span class=w> </span><span class=n>actual</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>decompressed</span><span class=p>.</span><span class=n>next</span><span class=p>().</span><span class=o>?</span><span class=p>;</span><span class=w>
</span><span class=w>        </span><span class=k>try</span><span class=w> </span><span class=n>testing</span><span class=p>.</span><span class=n>expectEqual</span><span class=p>(</span><span class=n>expected</span><span class=p>,</span><span class=w> </span><span class=n>actual</span><span class=p>);</span><span class=w>
</span><span class=w>    </span><span class=p>}</span><span class=w>
</span><span class=w></span><span class=p>}</span><span class=w>
</span></code></pre></div><h3 id=a-stream-of-op-codes>A stream of op codes</h3><p>I settled on a really simple idea: these data files all have basically just a variable number of integers per line. And if I kept &ldquo;registers&rdquo; representing the current value for each integer, I could determine the difference between the past line and the subsequent one to produce a difference. If I encoded that difference as a stream of opcodes with associative data, then to decompress the file I could simply &ldquo;replay&rdquo; those operations based on the opcodes and then iteratively come up with more finely-specified, specific opcodes to handle specific types of data.</p><p>I started out simple, really just with two opcodes:</p><div class=highlight><pre class=chroma><code class=language-Zig data-lang=Zig><span class=c1>// A UDDC opcode for a decomposition file.
</span><span class=c1></span><span class=kr>const</span><span class=w> </span><span class=n>Opcode</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>enum</span><span class=p>(</span><span class=n>u4</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span><span class=w>    </span><span class=c1>// Sets all the register values with no compression.
</span><span class=c1></span><span class=w>    </span><span class=n>set</span><span class=p>,</span><span class=w>
</span><span class=w>
</span><span class=w>    </span><span class=c1>// Denotes the end of the opcode stream. This is so that we don&#39;t need to encode the total
</span><span class=c1></span><span class=w>    </span><span class=c1>// number of opcodes in the stream up front (note also the file is bit packed: there may be
</span><span class=c1></span><span class=w>    </span><span class=c1>// a few remaining zero bits at the end as padding so we need an EOF opcode rather than say
</span><span class=c1></span><span class=w>    </span><span class=c1>// catching the actual file read EOF.)
</span><span class=c1></span><span class=w>    </span><span class=n>eof</span><span class=p>,</span><span class=w>
</span><span class=w></span><span class=p>};</span><span class=w>
</span></code></pre></div><p>Using these two opcodes, I was able to effectively encode the entire file. The <code>set</code> opcode had some associative data which effectively expressed an entire raw, uncompressed entry in the file (one line.) This increased the file size since it was effectively just adding 4 bits (the opcode) as additional overhead.</p><h3 id=iteratively-finding-the-most-lucrative-opcodes>Iteratively finding the most lucrative opcodes</h3><p>To find the most lucrative (i.e. compressed) opcodes, I printed the data I would associate with an opcode (like <code>set</code>) and then looked for repetitions. Sometimes manually, and sometimes by e.g. piping data to a combination of <code>sort|uniq -c|sort -r</code> to find common patterns.</p><p>Since I was printing <em>differences</em> between e.g. the current value and previous value, it was really easy to find common patterns that appeared in the file very frequently, such as specific fields incrementing by specific amounts with one field being arbitrary:</p><div class=highlight><pre class=chroma><code class=language-zig data-lang=zig><span class=w>    </span><span class=c1>// increments key[3] += 1; sets value.seq[0]; emits an entry.
</span><span class=c1></span><span class=w>    </span><span class=c1>// 1685 instances
</span><span class=c1></span><span class=w>    </span><span class=n>increment_key_3_and_set_value_seq_0_and_emit</span><span class=p>,</span><span class=w>
</span></code></pre></div><p>Once I had narrowed down to a larger group of opcodes that more specifically represented the data, I was able to print the number of bits required to store the change in specific fields (like <code>value.seq[0]</code>) and add even more specific opcodes to use variable bit widths:</p><div class=highlight><pre class=chroma><code class=language-zig data-lang=zig><span class=w>    </span><span class=c1>// increments key[3] += 1; sets value.seq[0]; emits an entry.
</span><span class=c1></span><span class=w>    </span><span class=c1>// 1685 instances
</span><span class=c1></span><span class=w>    </span><span class=n>increment_key_3_and_set_value_seq_0_2bit_and_emit</span><span class=p>,</span><span class=w> </span><span class=c1>// 978 instances, 2323 byte reduction
</span><span class=c1></span><span class=w>    </span><span class=n>increment_key_3_and_set_value_seq_0_8bit_and_emit</span><span class=p>,</span><span class=w> </span><span class=c1>// 269 instances, 437 byte reduction
</span><span class=c1></span><span class=w>    </span><span class=n>increment_key_3_and_set_value_seq_0_21bit_and_emit</span><span class=p>,</span><span class=w> </span><span class=c1>// 438 instances
</span></code></pre></div><p>It being a stream of opcodes was quite nice, because it allowed me to determine how much space was being consumed by a given opcode in sum and target further reducing the size of opcodes that took up the most space. It also made it really easy to find opcodes that I though <em>might</em> help, but in practice turned out to not be that frequent. Just print them, pipe to <code>sort|uniq -c|sort -r</code> to count them - and remove the lowest hanging fruit.</p><h3 id=a-stream-of-opcodes-for-a-state-machine-a-natural-progression-from-a-binary-format>A stream of opcodes for a state machine: a natural progression from a binary format?</h3><p>I chose an opcode stream for a reason: so that I could encode some complex logic in the form of a state machine. This came in handy for the <code>allkeys.txt</code> file in specific, as it allowed me to introduce <em>incrementors</em> into the mix which would <em>increment register values by a chosen amount each iteration (value &ldquo;emission&rdquo;)</em>.</p><p>The final opcodes for the allkeys.txt file ended up being:</p><div class=highlight><pre class=chroma><code class=language-zig data-lang=zig><span class=c1>// A UDDC opcode for an allkeys file.
</span><span class=c1></span><span class=kr>const</span><span class=w> </span><span class=n>Opcode</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=k>enum</span><span class=p>(</span><span class=n>u3</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span><span class=w>    </span><span class=c1>// Sets an incrementor for the key register, incrementing the key by this much on each emission.
</span><span class=c1></span><span class=w>    </span><span class=c1>// 10690 instances, 13,480.5 bytes
</span><span class=c1></span><span class=w>    </span><span class=n>inc_key</span><span class=p>,</span><span class=w>
</span><span class=w>
</span><span class=w>    </span><span class=c1>// Sets an incrementor for the value register, incrementing the value by this much on each emission.
</span><span class=c1></span><span class=w>    </span><span class=c1>// 7668 instances, 62,970 bytes
</span><span class=c1></span><span class=w>    </span><span class=n>inc_value</span><span class=p>,</span><span class=w>
</span><span class=w>
</span><span class=w>    </span><span class=c1>// Emits a single value.
</span><span class=c1></span><span class=w>    </span><span class=c1>// 31001 instances, 15,500.5 bytes
</span><span class=c1></span><span class=w>    </span><span class=n>emit_1</span><span class=p>,</span><span class=w>
</span><span class=w>    </span><span class=n>emit_2</span><span class=p>,</span><span class=w>
</span><span class=w>    </span><span class=n>emit_4</span><span class=p>,</span><span class=w>
</span><span class=w>    </span><span class=n>emit_8</span><span class=p>,</span><span class=w>
</span><span class=w>    </span><span class=n>emit_32</span><span class=p>,</span><span class=w>
</span><span class=w>
</span><span class=w>    </span><span class=c1>// Denotes the end of the opcode stream. This is so that we don&#39;t need to encode the total
</span><span class=c1></span><span class=w>    </span><span class=c1>// number of opcodes in the stream up front (note also the file is bit packed: there may be
</span><span class=c1></span><span class=w>    </span><span class=c1>// a few remaining zero bits at the end as padding so we need an EOF opcode rather than say
</span><span class=c1></span><span class=w>    </span><span class=c1>// catching the actual file read EOF.)
</span><span class=c1></span><span class=w>    </span><span class=n>eof</span><span class=p>,</span><span class=w>
</span><span class=w></span><span class=p>};</span><span class=w>
</span></code></pre></div><p>This meant I could determine the difference in the <code>key</code> and <code>value</code> fields (what those actually are isn&rsquo;t important, just that they are all minor incremental differences on the prior entry in the file) - set an <em>incrementor</em> to do some work on each emission, such as say increment the <code>key</code> array by <code>[0, 1, 5]</code> each emission, and then say &ldquo;now emit_32 values!&rdquo;.</p><p>Suddenly, instead of encoding 32 key entries (32 * 3 key values * 21 bits) I am just setting an incrementor (3 key values * 21 bits) and a single opcode to emit 32 values (3 bits).</p><p>Overall, this gave me a very nice, natural-feeling progression from a &ldquo;raw binary format&rdquo; to something a bit more specific - a bit more <em>compressed.</em></p><h2 id=results-better-than-gzipbrotli-and-even-better-_with_-them>Results? Better than gzip/brotli; and even better <em>with</em> them!</h2><p>For lack of better words, I&rsquo;ll call my compression algorithm here Unicode Data Differential Compression, since it&rsquo;s differential and specifically for the Unicode data table files - or UDDC for short.</p><p>The two files went from the original 568K (with gzip) down to just 61K (with UDDC+gzip). With this, we are able to equal or match both <code>gzip -9</code> and <code>brotli -9</code> on their own, AND when combined with gzip or brotli we are able to reduce by 40-70%:</p><table><thead><tr><th>File</th><th>Before (bytes)</th><th>After (bytes)</th><th>Change</th></tr></thead><tbody><tr><td><code>Decompositions.bin</code></td><td>48,242</td><td>19,072</td><td>-60.5% (-29,170 bytes)</td></tr><tr><td><code>Decompositions.bin.br</code></td><td>24,411</td><td>14,783</td><td>-39.4% (-9,628 bytes)</td></tr><tr><td><code>Decompositions.bin.gz</code></td><td>30,931</td><td>15,670</td><td>-49.34% (15,261 bytes)</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td><code>allkeys.bin</code></td><td>373,719</td><td>100,907</td><td>-73.0% (-272,812 bytes)</td></tr><tr><td><code>allkeys.bin.br</code></td><td>108,982</td><td>44,860</td><td>-58.8% (-64,122 bytes)</td></tr><tr><td><code>allkeys.bin.gz</code></td><td>163,237</td><td>46,996</td><td>-71.2% (-116,241 bytes)</td></tr></tbody></table><ul><li>Before represents binary format without UDDC compression.</li><li>After represents binary format with UDDC compression.</li><li><code>.br</code> represents <code>brotli -9 &lt;file></code> compression</li><li><code>.gz</code> represents <code>gzip -9 &lt;file></code> compression</li></ul><h3 id=why-test-with-gzipbrotli-but-not-others>Why test with gzip/brotli but not others?</h3><p>I chose to compare against gzip/brotli specifically because you get those effectively for free in WebAssembly: browsers already know how to decompress those and ship with gzip/brotli decompressors - so you can use them for free without shipping any additional code.</p><h3 id=how-complex-is-the-implementation>How complex is the implementation?</h3><p>The final implementation for both files is only a few hundred lines (excluding blank lines, comments, and tests):</p><ul><li><a href=https://github.com/jecolon/ziglyph/blob/main/src/collator/AllKeysFile.zig><code>AllKeysFile.zig</code></a>: 298 lines</li><li><a href=https://github.com/jecolon/ziglyph/blob/main/src/normalizer/DecompFile.zig><code>DecompFile.zig</code></a> 336 lines</li></ul><p>I have not measured produced machine code size yet, but suspect it is relatively negligible compared to the gains.</p><h2 id=notable-mention>Notable mention</h2><p>I should mention that the Unicode spec, as <a href=https://github.com/jecolon>@jecolon</a> pointed out to me, does suggest ways to reduce sort key lengths and implement Run-length Compression:</p><ul><li><a href=https://unicode.org/reports/tr10/#Reducing_Sort_Key_Lengths>https://unicode.org/reports/tr10/#Reducing_Sort_Key_Lengths</a></li><li><a href=https://unicode.org/reports/tr10/#Run-length_Compression>https://unicode.org/reports/tr10/#Run-length_Compression</a></li></ul><p>I wasn&rsquo;t able to locate an implementation of this (I&rsquo;d be curious to compare results!) but suspect that, as the run-length compression does not fit the data as tightly, it would not compress quite as well (although would handle any major changes to the type of data in the files without requiring compression algorithm changes better.)</p><p>Also of note is that their algorithm only seems to be mentioned in the context of allkeys.txt / the Unicode Collation Algorithm, not in the context of normalization/decompositions from <code>UnicodeData.txt</code>.</p><h2 id=conclusion>Conclusion</h2><p>Ask questions, stay curious, don&rsquo;t be afraid to experiment even if it&rsquo;s outside of your domain of expertise. You might surprise yourself and find something interesting, challenging, and worthwhile.</p></div></main><script>function addAnchor(a){a.insertAdjacentHTML('afterbegin',`<a href="#${a.id}" class="hanchor" ariaLabel="Anchor">#</a> `)}document.addEventListener('DOMContentLoaded',function(){var a=document.querySelectorAll('h1[id], h2[id], h3[id], h4[id]');a&&a.forEach(addAnchor)})</script></div><div class=footer><div class=row-1><a href=https://hexops.com/privacy>Privacy matters</a>
<a href=https://github.com/sponsors/slimsag>Sponsor on GitHub</a>
<a href=https://machengine.org>machengine.org</a></div><div class=row-2><a href=/feed.xml><img alt="RSS feed" src="https://shields.io/badge/RSS-follow-green?logo=RSS"></a></div><div class=row-3><a href=https://hexops.com><img class="logo color-auto" alt="Hexops logo" src=https://raw.githubusercontent.com/hexops/media/234e15f265b19743c580a078b2d68660c92675d4/logo.svg height=50px></a></div></div></body></html>