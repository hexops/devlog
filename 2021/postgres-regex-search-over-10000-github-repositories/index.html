<!doctype html><html><head><link rel=stylesheet href=https://devlog.hexops.com/assets/font/stylesheet.css><link rel=stylesheet href=https://devlog.hexops.com/main.959becff4f3640c2ba19521949a93832fa147f951930301c1c72e74e25823239.css><script async defer data-domain=hexops.com src=https://hexops.com/opendata.js></script><meta charset=utf-8><title>Postgres regex search over 10,000 GitHub repositories (using only a Macbook) | Hexops' devlog</title><link rel=canonical href=https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories/><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="Postgres regex search over 10,000 GitHub repositories (using only a Macbook)"><meta property="og:description" content="In this article, we share empirical measurements from our experiments in using Postgres to index and search over 10,000 top GitHub repositories using pg_trgm on only a Macbook."><meta property="og:type" content="article"><meta property="og:url" content="https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories/"><meta property="article:section" content="2021"><meta property="article:published_time" content="2021-02-17T00:00:00+00:00"><meta property="article:modified_time" content="2021-02-17T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Postgres regex search over 10,000 GitHub repositories (using only a Macbook)"><meta name=twitter:description content="In this article, we share empirical measurements from our experiments in using Postgres to index and search over 10,000 top GitHub repositories using pg_trgm on only a Macbook."></head><body><div class=navbar><div class=content><a href=/ class=logo><img src=https://raw.githubusercontent.com/hexops/media/234e15f265b19743c580a078b2d68660c92675d4/logo.svg>' devlog</a><div><a href=https://hexops.com/about class=item>About</a>
<a href=/archives class=item>Archives</a>
<a href=https://github.com/hexops class=item>GitHub</a></div></div></div><div id=content><main aria-role=main class=main-single><div class=single><header class=header><h1>Postgres regex search over 10,000 GitHub repositories (using only a Macbook)</h1><div class=metadata><time>February 17, 2021</time>
• <a class=category href=/categories/search>search</a>
• <a class=category href=/categories/trigrams>trigrams</a></div></header><p>In this article, we share empirical measurements from our experiments in using Postgres to index and search over 10,000 top GitHub repositories using <code>pg_trgm</code> on only a Macbook.</p><p>This is a follow up to <a href=https://devlog.hexops.com/2021/postgres-trigram-search-learnings>&ldquo;Postgres Trigram search learnings&rdquo;</a>, in which we shared several learnings and beliefs about trying to use Postgres Trigram indexes as an alterative to Google&rsquo;s <a href=https://github.com/google/zoekt>Zoekt</a> (&ldquo;Fast trigram based code search&rdquo;).</p><p>We share our results, as well as <a href=https://github.com/hexops/pgtrgm_emperical_measurements>the exact steps we performed, scripts, and lists of the top 20,000 repositories by stars/language on GitHub</a> so you can reproduce the results yourself should you desire.</p><h2 id=tldr>TL;DR</h2><p><strong>This article is extensive and more akin to a research paper than a blog post.</strong> If you&rsquo;re interested in our conclusions, see <a href=#conclusions>conclusions</a> instead.</p><h2 id=goals>Goals</h2><p>We wanted to get empirical measurements for how suitable Postgres is in providing regexp search over documents, e.g. as an alterative to Google&rsquo;s <a href=https://github.com/google/zoekt>Zoekt</a> (&ldquo;Fast trigram based code search&rdquo;). In specific:</p><ul><li>How many repositories can we index on just a 2019 Macbook Pro?</li><li>How fast are different regexp searches over the corpus?</li><li>What Postgres 13 configuration gives best results?</li><li>What other operational effects need consideration if seriously attempting to use Postgres as the backend for a regexp search engine?</li><li>What is the best database schema to use?</li></ul><h2 id=hardware>Hardware</h2><p>We ran all tests on a 2019 Macbook Pro with:</p><ul><li>2.3 GHz 8-Core Intel Core i9</li><li>16 GB 2667 MHz DDR4</li></ul><p>During test execution, few other Mac applications were in use such that effectively all CPU/memory was available to Postgres.</p><h2 id=corpus>Corpus</h2><p>We scraped <a href=https://github.com/hexops/pgtrgm_emperical_measurements/tree/main/top_repos>lists of the top 1,000 repositories from the GitHub search API</a> ranked by stars for each of the following languages (~20.5k repositories in total):</p><ul><li>C++, C#, CSS, Go, HTML, Java, JavaScript, MatLab, ObjC, Perl, PHP, Python, Ruby, Rust, Shell, Solidity, Swift, TypeScript, VB .NET, and Zig.</li></ul><p>Cloning all ~20.5k repositories in parallel took ~14 hours with a fast ~100 Mbps connection to GitHub&rsquo;s servers.</p><h3 id=dataset-reduction>Dataset reduction</h3><p>We found the amount of disk space required by <code>git clone --depth 1</code> on these repositories to be a sizable ~412G for just 12,148 repositories - and so we put in place several processes for further reduce the dataset size by about 66%:</p><ul><li>Removing <code>.git</code> directories resulted in a 30% reduction (412G -> 290G, for 12,148 repositories)</li><li>Removing files > 1 MiB resulted in another 51% reduction (290G -> 142G, for 12,148 repositories - note GitHub does not index files > 384 KiB in their search engine)</li></ul><h2 id=database-insertion>Database insertion</h2><p>We <a href=https://github.com/hexops/pgtrgm_emperical_measurements/blob/main/cmd/corpusindex/main.go>concurrently inserted</a> the entire corpus into Postgres, with the following DB schema:</p><div class=highlight><pre class=chroma><code class=language-sql data-lang=sql><span class=k>CREATE</span><span class=w> </span><span class=n>EXTENSION</span><span class=w> </span><span class=k>IF</span><span class=w> </span><span class=k>NOT</span><span class=w> </span><span class=k>EXISTS</span><span class=w> </span><span class=n>pg_trgm</span><span class=p>;</span><span class=w>
</span><span class=w></span><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=k>IF</span><span class=w> </span><span class=k>NOT</span><span class=w> </span><span class=k>EXISTS</span><span class=w> </span><span class=n>files</span><span class=w> </span><span class=p>(</span><span class=w>
</span><span class=w>    </span><span class=n>id</span><span class=w> </span><span class=n>bigserial</span><span class=w> </span><span class=k>PRIMARY</span><span class=w> </span><span class=k>KEY</span><span class=p>,</span><span class=w>
</span><span class=w>    </span><span class=n>contents</span><span class=w> </span><span class=nb>text</span><span class=w> </span><span class=k>NOT</span><span class=w> </span><span class=k>NULL</span><span class=p>,</span><span class=w>
</span><span class=w>    </span><span class=n>filepath</span><span class=w> </span><span class=nb>text</span><span class=w> </span><span class=k>NOT</span><span class=w> </span><span class=k>NULL</span><span class=w>
</span><span class=w></span><span class=p>);</span><span class=w>
</span></code></pre></div><p>In total, this took around ~8 hours to complete and Postgres&rsquo;s entire on-disk utilization was 101G.</p><h2 id=creating-the-trigram-index>Creating the Trigram index</h2><p>We tried three separate times to index the dataset using the following GIN Trigram index:</p><pre><code>CREATE INDEX IF NOT EXISTS files_contents_trgm_idx ON files USING GIN (contents gin_trgm_ops);
</code></pre><ul><li><strong>In the first attempt, we hit an OOM after 11 hours and 34 minutes.</strong> This was due to a rapid spike in memory usage at the very end of indexing. We used a <a href=https://github.com/hexops/pgtrgm_emperical_measurements#configuration-attempt-1-indexing-failure-oom>fairly aggressive</a> Postgres configuration with a very large max WAL size, so it was not entirely unexpected.</li><li><strong>In the second attempt, we ran out of SSD disk space after ~27 hours</strong>. Notable is that the disk space largely grew towards the end of indexing, similar to when we faced an OOM - it was not a gradual increase over time. For this attempt, we used the excellent <a href=https://pgtune.leopard.in.ua/#/>pgtune</a> tool to reduce our first Postgres configuration as follows:</li></ul><pre><code>shared_buffers = 4GB → 2560MB
effective_cache_size = 12GB → 7680MB
maintenance_work_mem = 16GB → 1280MB
default_statistics_target = 100 → 500
work_mem = 5242kB → 16MB
min_wal_size = 50GB → 4GB
max_wal_size = 4GB → 16GB
max_parallel_workers_per_gather = 8 → 4
max_parallel_maintenance_workers = 8 → 4
</code></pre><ul><li><strong>In our third and final attempt, we cut the dataset in half and indexing succeeded after 22 hours.</strong> In specific, we deleted half of the files in the database (from 19,441,820 files / 178GiB of data to 9,720,910 files / 82 GiB of data.) The Postgres configuration used was the same as in attempt 2.</li></ul><h2 id=indexing-performance-memory-usage>Indexing performance: Memory usage</h2><p>In our first attempt, we see the reported <code>docker stats</code> memory usage of the container grow up to 12 GiB (chart shows MiB of memory used over time):</p><img width=981 alt=image src=https://user-images.githubusercontent.com/3173176/107313722-56bbac80-6a50-11eb-94c7-8e13ea095053.png><p>In our second and third attempts, we see far less memory usage (~1.6 GiB consistently):</p><img width=980 alt=image src=https://user-images.githubusercontent.com/3173176/107314104-350ef500-6a51-11eb-909f-2f1b524d29b2.png>
<img width=980 alt=image src=https://user-images.githubusercontent.com/3173176/107315387-ce3f0b00-6a53-11eb-886c-410f000f73bd.png><h2 id=indexing-performance-cpu-usage>Indexing performance: CPU usage</h2><p>Postgres' Trigram indexing appears to be mostly single-threaded (at least when indexing <em>a single table</em>, we test multiple tables later.)</p><p>In our first attempt, CPU usage for the container did not rise above 156% (one and a half virtual CPU cores):</p><img width=982 alt=image src=https://user-images.githubusercontent.com/3173176/107313915-cc277d00-6a50-11eb-9282-62159a127966.png><p>Our second attempt was around 150-200% CPU usage on average:</p><img width=980 alt=image src=https://user-images.githubusercontent.com/3173176/107314168-507a0000-6a51-11eb-8a18-ec18752f7f16.png><p>Our third attempt similarly saw an average of 150-200%, but with a brief spike towards the end to ~350% CPU:</p><img width=980 alt=image src=https://user-images.githubusercontent.com/3173176/107315239-8324f800-6a53-11eb-9a5b-fcc61d1a7b59.png><h2 id=indexing-performance-disk-io>Indexing performance: Disk IO</h2><p>Disk reads/writes during indexing averaged about ~250 MB/s for reads (blue) and writes (red). Native in-software tests show the same Macbook able to achieve read/write speeds of ~860 MB/s with &lt;5% affect on CPU utilization.</p><p><small>Addition made Feb 20, 2021:</small> We ran tests using native Postgres as well (instead of in Docker with a bind mount) and found better indexing and query performance, more on this below.</p><img width=599 alt=image src=https://user-images.githubusercontent.com/3173176/106507903-ec6f9e80-6488-11eb-88a8-78e5b7aacfd6.png><h2 id=indexing-performance-disk-space>Indexing performance: Disk space</h2><p>The database contains 9,720,910 files totalling 82.07 GiB:</p><pre><code>postgres=# select count(filepath) from files;
  count  
---------
 9720910
(1 row)

postgres=# select SUM(octet_length(contents)) from files;
     sum     
-------------
 88123563320
(1 row)
</code></pre><p><strong>Before indexing</strong>, we find that all of Postgres is consuming 54G:</p><pre><code>$ du -sh .postgres/
 54G	.postgres/
</code></pre><p>After <code>CREATE INDEX</code>, Postgres uses:</p><pre><code>$ du -sh .postgres/
 73G	.postgres/
</code></pre><p>Thus, the index size for 82 GiB of text is 19 GiB (or 23% of the data size.)</p><h2 id=database-startup-times>Database startup times</h2><p>From an operational standpoint, it is worth noting that if Postgres is starting clean (i.e. previous shutdown was graceful) then startup time is almost instantaneous: it begins accepting connections immediately and loads the index as needed.</p><p>However, if Postgres experienced a non-graceful termination during e.g. startup, it can take a hefty ~10 minutes with this dataset to start as it goes through an automated recovery process.</p><h2 id=queries-executed>Queries executed</h2><p>In total, we executed 19,936 search queries against the index. We chose queries which we expect give reasonably varying amounts of coverage over the trigram index (that is, queries whose trigrams are more or less likely to occur in many files):</p><table><thead><tr><th>Regexp query</th><th>Matching # files in entire dataset</th></tr></thead><tbody><tr><td><code>var</code></td><td>unknown (2m+ suspected)</td></tr><tr><td><code>error</code></td><td>1,479,452</td></tr><tr><td><code>123456789</code></td><td>59,841</td></tr><tr><td><code>fmt\.Error</code></td><td>127,895</td></tr><tr><td><code>fmt\.Println</code></td><td>22,876</td></tr><tr><td><code>bytes.Buffer</code></td><td>34,554</td></tr><tr><td><code>fmt\.Print.*</code></td><td>37,319</td></tr><tr><td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td><td>0</td></tr><tr><td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td><td>0</td></tr></tbody></table><details><summary>Detailed breakdown</summary><div markdown=1><table><thead><tr><th>Query</th><th>Result Limit</th><th>Times executed</th></tr></thead><tbody><tr><td><code>var</code></td><td>10</td><td>1000</td></tr><tr><td><code>var</code></td><td>100</td><td>1000</td></tr><tr><td><code>var</code></td><td>1000</td><td>100</td></tr><tr><td><code>var</code></td><td>unlimited</td><td>4</td></tr><tr><td><code>error'</code></td><td>10</td><td>2000</td></tr><tr><td><code>error'</code></td><td>100</td><td>2000</td></tr><tr><td><code>error'</code></td><td>1000</td><td>200</td></tr><tr><td><code>error'</code></td><td>unlimited</td><td>18</td></tr><tr><td><code>123456789</code></td><td>10</td><td>1000</td></tr><tr><td><code>123456789</code></td><td>100</td><td>1000</td></tr><tr><td><code>123456789</code></td><td>1000</td><td>100</td></tr><tr><td><code>123456789</code></td><td>unlimited</td><td>2</td></tr><tr><td><code>fmt\.Error</code></td><td>10</td><td>1000</td></tr><tr><td><code>fmt\.Error</code></td><td>100</td><td>1000</td></tr><tr><td><code>fmt\.Error</code></td><td>1000</td><td>100</td></tr><tr><td><code>fmt\.Error</code></td><td>unlimited</td><td>2</td></tr><tr><td><code>fmt\.Println</code></td><td>10</td><td>1000</td></tr><tr><td><code>fmt\.Println</code></td><td>100</td><td>1000</td></tr><tr><td><code>fmt\.Println</code></td><td>1000</td><td>100</td></tr><tr><td><code>fmt\.Println</code></td><td>unlimited</td><td>2</td></tr><tr><td><code>bytes.Buffer</code></td><td>10</td><td>4</td></tr><tr><td><code>bytes.Buffer</code></td><td>100</td><td>4</td></tr><tr><td><code>bytes.Buffer</code></td><td>1000</td><td>4</td></tr><tr><td><code>bytes.Buffer</code></td><td>unlimited</td><td>2</td></tr><tr><td><code>fmt\.Print.*</code></td><td>10</td><td>1000</td></tr><tr><td><code>fmt\.Print.*</code></td><td>100</td><td>1000</td></tr><tr><td><code>fmt\.Print.*</code></td><td>1000</td><td>100</td></tr><tr><td><code>fmt\.Print.*</code></td><td>unlimited</td><td>2</td></tr><tr><td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td><td>10</td><td>1000</td></tr><tr><td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td><td>100</td><td>1000</td></tr><tr><td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td><td>1000</td><td>100</td></tr><tr><td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td><td>unlimited</td><td>2</td></tr><tr><td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td><td>10</td><td>1000</td></tr><tr><td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td><td>100</td><td>1000</td></tr><tr><td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td><td>1000</td><td>100</td></tr><tr><td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td><td>unlimited</td><td>2</td></tr></tbody></table></div></details><h2 id=query-performance>Query performance</h2><p>In total, we executed 19,936 search queries against the database (linearly, not in parallel) which completed in the following times:</p><table><thead><tr><th>Time bucket</th><th>Percentage of queries</th><th>Number of queries</th></tr></thead><tbody><tr><td>Under 50ms</td><td>30%</td><td>5,933</td></tr><tr><td>Under 250ms</td><td>41%</td><td>8,088</td></tr><tr><td>Under 500ms</td><td>52%</td><td>10,275</td></tr><tr><td>Under 750ms</td><td>63%</td><td>12,473</td></tr><tr><td>Under 1s</td><td>68%</td><td>13,481</td></tr><tr><td>Under 1.5s</td><td>74%</td><td>14,697</td></tr><tr><td>Under 3s</td><td>79%</td><td>15,706</td></tr><tr><td>Under 25s</td><td>79%</td><td>15,708</td></tr><tr><td>Under 30s</td><td>99%</td><td>19,788</td></tr></tbody></table><h2 id=query-performance-vs-planning-time>Query performance vs. planning time</h2><p>The following scatter plot shows how 79% of queries executed in under 3s (Y axis, in ms), while Postgres&rsquo;s query planner had planned them for execution in under 100-250ms generally (X axis, in ms):</p><img width=1252 alt=image src=https://user-images.githubusercontent.com/3173176/107848471-ef379100-6db0-11eb-8396-4d156a179aae.png><p>If we expand the view to include all queries, we start to get a picture of just how outlier these 21% of queries are (note that the small block of dots in the bottom left represents the same diagram shown above):</p><img width=1250 alt=image src=https://user-images.githubusercontent.com/3173176/107848517-3cb3fe00-6db1-11eb-9652-e65d7d88fe36.png><h2 id=query-time-vs-cpu--memory-usage>Query time vs. CPU & Memory usage</h2><p>The following image shows:</p><ul><li>(top) Query time in milliseconds</li><li>(middle) CPU usage percentage (e.g. 801% refers to 8 out of 16 virtual CPU cores being consumed)</li><li>(bottom) Memory usage in MiB.</li></ul><img width=1255 alt=image src=https://user-images.githubusercontent.com/3173176/107848716-efd12700-6db2-11eb-8e8b-a8141a6bdb0b.png><p>Notable insights from this are:</p><ul><li>The large increase in resource usage towards the end is when we began executing queries with no <code>LIMIT</code>.</li><li>CPU usage does not exceed 138%, until the spike at the end.</li><li>Memory usage does not exceed 42 MiB, until the spike at the end.</li></ul><p>We suspect <code>pg_trgm</code> is single-threaded within the scope of a single table, but with <a href=https://www.postgresql.org/docs/10/ddl-partitioning.html>table data partitioning</a> (or splitting data into multiple tables with subsets of the data), we suspect better parallelism could be achieved.</p><h2 id=investigating-slow-queries>Investigating slow queries</h2><p>If we plot the number of index rechecks (X axis) vs. execution time (Y axis), we can clearly see one of the most significant aspects of slow queries is that they have many more index rechecks:</p><img width=1036 alt=image src=https://user-images.githubusercontent.com/3173176/107849660-fc0cb280-6db9-11eb-9c10-cb7e74366ab7.png><p>And if we look at <a href=https://github.com/hexops/pgtrgm_emperical_measurements/blob/main/query_logs/query-run-3.log#L3-L24>the <code>EXPLAIN ANALYZE</code> output for one of these queries</a> we can also confirm <code>Parallel Bitmap Heap Scan</code> is slow due to <code>Rows Removed by Index Recheck</code>.</p><h2 id=table-splitting>Table splitting</h2><p>Splitting up the search index into multiple smaller tables seems like an obvious approach to getting <code>pg_trgm</code> to use multiple CPU cores. We tried this by taking the same exact data set and splitting it into 200 tables, and found numerous benefits:</p><h3 id=benefit-1-incremental-indexing>Benefit 1: Incremental indexing</h3><p>If indexing fails after 11-27 hours, as happened to us twice in the non-splitting approach, all progress is not lost.</p><h3 id=benefit-2-parallel-indexing>Benefit 2: Parallel indexing</h3><p>Unlike our first non-splitting approach, which showed we were only able to utilize 1.5-2 virtual CPU cores, with multiple tables we are able to utilize 8-9 virtual CPU cores:</p><img width=1143 alt=image src=https://user-images.githubusercontent.com/3173176/108118901-3b5a2e00-705c-11eb-86d1-a7828517b2e8.png><h3 id=benefit-3-indexing-is-84-faster>Benefit 3: Indexing is 84% faster</h3><p>Unlike our first attempt which took 22 hours in total, parallel indexing completed in only 3h27m.</p><h3 id=benefit-4-indexing-uses-69-less-memory>Benefit 4: Indexing uses 69% less memory</h3><p>With non-splitting we saw peak memory usage up to 12 GiB. With the same exact Postgres configuration, we were able to index with only 3.7 GiB peak memory usage:</p><img width=1140 alt=image src=https://user-images.githubusercontent.com/3173176/108119244-bd4a5700-705c-11eb-949b-69828acd7c7c.png><h2 id=benefit-4-parallel-querying>Benefit 4: Parallel querying</h2><p>Previously, we saw CPU utilization of only 138% (1.3 virtual CPU cores), with table splitting we see CPU utilization during queries of 1600% (16 virtual CPU cores) showing we are doing work fully in parallel:</p><img width=1144 alt=image src=https://user-images.githubusercontent.com/3173176/108114005-79078880-7055-11eb-9f55-bc4ca65c4808.png><p>Similarly, we saw memory usage average around ~380 MiB, compared to only ~42 MiB before:</p><img width=1143 alt=image src=https://user-images.githubusercontent.com/3173176/108115193-04354e00-7057-11eb-9782-8d3125c122e1.png><h2 id=benefit-5-query-performance>Benefit 5: Query performance</h2><p>We reran the same exact set of search queries, but a smaller number of times overall (350 queries, instead of 19.9k - which we found to still be a representative enough sample.)</p><p>As we can see below, table splitting in general led to a 200-300% improvement in query time for heavier queries that previously took 20-30s, now taking only 7-15s thanks to parallel querying (top chart is before, bottom is after, both in milliseconds):</p><img width=1143 alt=image src=https://user-images.githubusercontent.com/3173176/108156053-3d90ac80-709d-11eb-8f81-b93456b54a41.png><p>We also grouped queries based on the <code>LIMIT</code> specified in the query and placed them into time buckets (&ldquo;how many queries completed in under 50ms?") - comparing the two shows that less complex queries and/or queries for fewer results were negatively affected slightly, while larger queries were helped substantially:</p><table><thead><tr><th>Change (positive is good)</th><th>Results limit</th><th>Bucket</th><th><strong>Queries in bucket before</strong></th><th><strong>Queries in bucket after</strong></th></tr></thead><tbody><tr><td>-33%</td><td>10</td><td>&lt;50ms</td><td>33%</td><td>0%</td></tr><tr><td>+13%</td><td>10</td><td>&lt;250ms</td><td>44%</td><td>57%</td></tr><tr><td>+33%</td><td>10</td><td>&lt;1s</td><td>77%</td><td>100%</td></tr><tr><td>-29%</td><td>100</td><td>&lt;100ms</td><td>29%</td><td>0%</td></tr><tr><td>+20%</td><td>100</td><td>&lt;500ms</td><td>50%</td><td>70%</td></tr><tr><td>+19%</td><td>100</td><td>&lt;10s</td><td>80%</td><td>99%</td></tr><tr><td>-12%</td><td>1000</td><td>&lt;250ms</td><td>12%</td><td>0%</td></tr><tr><td>-13%</td><td>1000</td><td>&lt;2.5s</td><td>77%</td><td>64%</td></tr><tr><td>+23%</td><td>1000</td><td>&lt;20s</td><td>77%</td><td>100%</td></tr><tr><td>+4%</td><td>none</td><td>&lt;20s</td><td>0%</td><td>4%</td></tr><tr><td>+18%</td><td>none</td><td>&lt;60s</td><td>0%</td><td>18%</td></tr></tbody></table><p>Detailed comparisons are available below for those interested:</p><details><summary>Queries with `LIMIT 10`</summary><div markdown=1><table><thead><tr><th>Time bucket</th><th>Percentage of queries (before)</th><th>Percentage of queries (after splitting)</th></tr></thead><tbody><tr><td>50ms</td><td>33.00% (2999 of 9004)</td><td>0% (0 of 100)</td></tr><tr><td>100ms</td><td>33.00% (2999 of 9004)</td><td>1.00% (1 of 100)</td></tr><tr><td>250ms</td><td>44.00% (3999 of 9004)</td><td>57.00% (57 of 100)</td></tr><tr><td>500ms</td><td>55.00% (4999 of 9004)</td><td>79.00% (79 of 100)</td></tr><tr><td>1000ms</td><td>77.00% (6998 of 9004)</td><td>80.00% (80 of 100)</td></tr><tr><td>2500ms</td><td>77.00% (7003 of 9004)</td><td>80.00% (80 of 100)</td></tr><tr><td>5000ms</td><td>77.00% (7004 of 9004)</td><td>80.00% (80 of 100)</td></tr><tr><td>10000ms</td><td>77.00% (7004 of 9004)</td><td>100.00% (100 of 100)</td></tr><tr><td>20000ms</td><td>77.00% (7004 of 9004)</td><td>100.00% (100 of 100)</td></tr><tr><td>30000ms</td><td>99.00% (8985 of 9004)</td><td>100.00% (100 of 100)</td></tr><tr><td>40000ms</td><td>99.00% (9003 of 9004)</td><td>100.00% (100 of 100)</td></tr><tr><td>50000ms</td><td>100.00% (9004 of 9004)</td><td>100.00% (100 of 100)</td></tr><tr><td>60000ms</td><td>100.00% (9004 of 9004)</td><td>100.00% (100 of 100)</td></tr></tbody></table></div></details><details><summary>Queries with `LIMIT 100`</summary><div markdown=1><table><thead><tr><th>Time bucket</th><th>Percentage of queries (before)</th><th>Percentage of queries (after splitting)</th></tr></thead><tbody><tr><td>50ms</td><td>29.00% (2934 of 10000)</td><td>0% (0 of 100)</td></tr><tr><td>100ms</td><td>29.00% (2978 of 10000)</td><td>0% (0 of 100)</td></tr><tr><td>250ms</td><td>39.00% (3975 of 10000)</td><td>31.00% (31 of 100)</td></tr><tr><td>500ms</td><td>50.00% (5000 of 10000)</td><td>70.00% (70 of 100)</td></tr><tr><td>1000ms</td><td>59.00% (5984 of 10000)</td><td>79.00% (79 of 100)</td></tr><tr><td>2500ms</td><td>79.00% (7996 of 10000)</td><td>80.00% (80 of 100)</td></tr><tr><td>5000ms</td><td>80.00% (8000 of 10000)</td><td>80.00% (80 of 100)</td></tr><tr><td>10000ms</td><td>80.00% (8000 of 10000)</td><td>99.00% (99 of 100)</td></tr><tr><td>20000ms</td><td>80.00% (8000 of 10000)</td><td>100.00% (100 of 100)</td></tr><tr><td>30000ms</td><td>99.00% (9999 of 10000)</td><td>100.00% (100 of 100)</td></tr><tr><td>40000ms</td><td>100.00% (10000 of 10000)</td><td>100.00% (100 of 100)</td></tr><tr><td>50000ms</td><td>100.00% (10000 of 10000)</td><td>100.00% (100 of 100)</td></tr><tr><td>60000ms</td><td>100.00% (10000 of 10000)</td><td>100.00% (100 of 100)</td></tr></tbody></table></div></details><details><summary>Queries with `LIMIT 1000`</summary><div markdown=1><table><thead><tr><th>Time bucket</th><th>Percentage of queries (before)</th><th>Percentage of queries (after splitting)</th></tr></thead><tbody><tr><td>50ms</td><td>0% (0 of 904)</td><td>0% (0 of 100)</td></tr><tr><td>100ms</td><td>0% (1 of 904)</td><td>0% (0 of 100)</td></tr><tr><td>250ms</td><td>12.00% (114 of 904)</td><td>0% (0 of 100)</td></tr><tr><td>500ms</td><td>30.00% (276 of 904)</td><td>21.00% (21 of 100)</td></tr><tr><td>1000ms</td><td>55.00% (499 of 904)</td><td>41.00% (41 of 100)</td></tr><tr><td>2500ms</td><td>77.00% (700 of 904)</td><td>64.00% (64 of 100)</td></tr><tr><td>5000ms</td><td>77.00% (704 of 904)</td><td>77.00% (77 of 100)</td></tr><tr><td>10000ms</td><td>77.00% (704 of 904)</td><td>98.00% (98 of 100)</td></tr><tr><td>20000ms</td><td>77.00% (704 of 904)</td><td>100.00% (100 of 100)</td></tr><tr><td>30000ms</td><td>88.00% (804 of 904)</td><td>100.00% (100 of 100)</td></tr><tr><td>40000ms</td><td>99.00% (901 of 904)</td><td>100.00% (100 of 100)</td></tr><tr><td>50000ms</td><td>99.00% (903 of 904)</td><td>100.00% (100 of 100)</td></tr><tr><td>60000ms</td><td>100.00% (904 of 904)</td><td>100.00% (100 of 100)</td></tr></tbody></table></div></details><details><summary>Queries with no limit`</summary><div markdown=1><table><thead><tr><th>Time bucket</th><th>Percentage of queries (before)</th><th>Percentage of queries (after splitting)</th></tr></thead><tbody><tr><td>50ms</td><td>0% (0 of 28)</td><td>0% (0 of 50)</td></tr><tr><td>100ms</td><td>0% (0 of 28)</td><td>0% (0 of 50)</td></tr><tr><td>250ms</td><td>0% (0 of 28)</td><td>0% (0 of 50)</td></tr><tr><td>500ms</td><td>0% (0 of 28)</td><td>0% (0 of 50)</td></tr><tr><td>1000ms</td><td>0% (0 of 28)</td><td>0% (0 of 50)</td></tr><tr><td>2500ms</td><td>0% (0 of 28)</td><td>0% (0 of 50)</td></tr><tr><td>5000ms</td><td>0% (0 of 28)</td><td>0% (0 of 50)</td></tr><tr><td>10000ms</td><td>0% (0 of 28)</td><td>0% (0 of 50)</td></tr><tr><td>20000ms</td><td>0% (0 of 28)</td><td>4.00% (2 of 50)</td></tr><tr><td>30000ms</td><td>0% (0 of 28)</td><td>16.00% (8 of 50)</td></tr><tr><td>40000ms</td><td>0% (0 of 28)</td><td>16.00% (8 of 50)</td></tr><tr><td>50000ms</td><td>0% (0 of 28)</td><td>18.00% (9 of 50)</td></tr><tr><td>60000ms</td><td>0% (0 of 28)</td><td>18.00% (9 of 50)</td></tr></tbody></table></div></details><h2 id=postgres-in-docker-vs-native-postgres>Postgres-in-Docker vs. native Postgres</h2><p><small>Addition made Feb 20, 2021</small></p><p>In our original article we did not clarify the performance impacts of running Postgres inside of Docker with a volume bind mount. This was raised as a potential source of IO performance difference to us by <a href=https://twitter.com/thorstenball>Thorsten Ball</a>.</p><p>We ran all tests above with Postgres in Docker, using a volume bind mount (the osxfs driver, not the experimental FUSE gRPC driver.)</p><p>We additionally ran the same table-splitting benchmarks on a native Postgres server (<a href=https://github.com/hexops/pgtrgm_emperical_measurements#native-postgres-tests>reproduction steps here</a>) and found the following key changes:</p><h3 id=cpu-usage--memory-usage-approximately-the-same>CPU usage & memory usage: approximately the same</h3><p>CPU and memory usage was approximately the same as in our Docker Postgres tests.</p><p>We anticipated this would be the case as the Macbook does have VT-x virtualization enabled (default on all i7/i9 Macbooks, and confirmed through <code>sysctl kern.hv_support</code>)</p><h3 id=indexing-speed-was-88-faster>Indexing speed was ~88% faster</h3><p>Running the statements to split up the large table into multiple smaller ones, i.e.:</p><div class=highlight><pre class=chroma><code class=language-sql data-lang=sql><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>files_000</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=k>SELECT</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>files</span><span class=w> </span><span class=k>WHERE</span><span class=w> </span><span class=n>id</span><span class=w> </span><span class=o>&gt;</span><span class=w> </span><span class=mi>0</span><span class=w> </span><span class=k>AND</span><span class=w> </span><span class=n>id</span><span class=w> </span><span class=o>&lt;</span><span class=w> </span><span class=mi>50000</span><span class=p>;</span><span class=w>
</span><span class=w></span><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>files_001</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=k>SELECT</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=k>FROM</span><span class=w> </span><span class=n>files</span><span class=w> </span><span class=k>WHERE</span><span class=w> </span><span class=n>id</span><span class=w> </span><span class=o>&gt;</span><span class=w> </span><span class=mi>50000</span><span class=w> </span><span class=k>AND</span><span class=w> </span><span class=n>id</span><span class=w> </span><span class=o>&lt;</span><span class=w> </span><span class=mi>100000</span><span class=p>;</span><span class=w>
</span><span class=w></span><span class=p>...</span><span class=w>
</span></code></pre></div><p>Was much faster in native Postgres, taking about 2-8s for each table instead of 20-40s previously, and taking only 15m in total instead of 2h before.</p><p>Parallel creation of the Trigram indexes using e.g.:</p><div class=highlight><pre class=chroma><code class=language-sql data-lang=sql><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w> </span><span class=k>IF</span><span class=w> </span><span class=k>NOT</span><span class=w> </span><span class=k>EXISTS</span><span class=w> </span><span class=n>files_000_contents_trgm_idx</span><span class=w> </span><span class=k>ON</span><span class=w> </span><span class=n>files</span><span class=w> </span><span class=k>USING</span><span class=w> </span><span class=n>GIN</span><span class=w> </span><span class=p>(</span><span class=n>contents</span><span class=w> </span><span class=n>gin_trgm_ops</span><span class=p>);</span><span class=w>
</span></code></pre></div><p>Was also much faster, taking only 23m compared to ~3h with Docker.</p><h3 id=query-performance-is-12-99-faster-depending-on-query>Query performance is 12-99% faster, depending on query</h3><p>We re-ran the same 350 queries as in our earlier table-splitting benchmark, and found the following substantial improvements:</p><ol><li>Queries that were previously very slow noticed a ~12% improvement. This is likely due to IO operations needed when interfacing with the 200 separate tables.</li><li>Queries that were previously in the middle-ground noticed meager ~5% improvements.</li><li>Queries that were previously fairly fast (likely searching only over a one or two tables before returning) noticed substantial 16-99% improvements.</li></ol><details><summary>Exhaustive comparison details (negative change is good)</summary><div markdown=1><table><thead><tr><th>Change</th><th>Time bucket</th><th>Queries under bucket <strong>before</strong></th><th>Queries under bucket <strong>after</strong></th></tr></thead><tbody><tr><td>0%</td><td>500s</td><td>350 of 350</td><td>350 of 350</td></tr><tr><td>-12%</td><td>100s</td><td>309 of 350</td><td>350 of 350</td></tr><tr><td>-12%</td><td>50s</td><td>309 of 350</td><td>350 of 350</td></tr><tr><td>-12%</td><td>40s</td><td>308 of 350</td><td>350 of 350</td></tr><tr><td>-12%</td><td>30s</td><td>308 of 350</td><td>349 of 350</td></tr><tr><td>-7%</td><td>25s</td><td>307 of 350</td><td>330 of 350</td></tr><tr><td>-7%</td><td>25s</td><td>307 of 350</td><td>330 of 350</td></tr><tr><td>-8%</td><td>20s</td><td>302 of 350</td><td>330 of 350</td></tr><tr><td>-8%</td><td>20s</td><td>302 of 350</td><td>330 of 350</td></tr><tr><td>-5%</td><td>10s</td><td>297 of 350</td><td>311 of 350</td></tr><tr><td>-26%</td><td>5s</td><td>237 of 350</td><td>319 of 350</td></tr><tr><td>-7%</td><td>2500ms</td><td>224 of 350</td><td>240 of 350</td></tr><tr><td>-9%</td><td>2000ms</td><td>219 of 350</td><td>240 of 350</td></tr><tr><td>-9%</td><td>1500ms</td><td>219 of 350</td><td>240 of 350</td></tr><tr><td>-16%</td><td>1000ms</td><td>200 of 350</td><td>237 of 350</td></tr><tr><td>-14%</td><td>750ms</td><td>190 of 350</td><td>221 of 350</td></tr><tr><td>-23%</td><td>500ms</td><td>170 of 350</td><td>220 of 350</td></tr><tr><td>-59%</td><td>250ms</td><td>88 of 350</td><td>217 of 350</td></tr><tr><td>-99%</td><td>100ms</td><td>1 of 350</td><td>168 of 350</td></tr><tr><td>-99%</td><td>50ms</td><td>1 of 350</td><td>168 of 350</td></tr></tbody></table></div></details><h2 id=conclusions>Conclusions</h2><p>We think the following learnings are most important:</p><ul><li><code>.git</code> directories, even with <code>--depth=1</code> clones, account for 30% of a repositories size on disk (at least in top 10,000 GitHub repositories.)</li><li>Files > 1 MiB (often binaries) account for another 51% of the data size on disk of repositories.</li><li>On only a Macbook Pro, it is possible to get Postgres Trigram regex search over 10,000 repositories to run most reasonable queries in under 5s - and certainly much faster with more hardware.</li><li><code>pg_trgm</code> performs single-threaded indexing and querying, unless you split your data up into multiple tables.</li><li>By default, a Postgres <code>text</code> colum will be compressed by Postgres on disk out of the box - resulting in a 23% reduction in size (with the files we inserted.)</li><li><code>pg_trgm</code> GIN indexes take around 26% the size of your data on disk. So if indexing 1 GiB of raw text, expect Postgres to store that text in around ~827 MiB plus 279 MiB for the GIN trigram index.</li><li>Splitting your data into multiple tables if using <code>pg_trgm</code> is an obvious win, as it allows for paralle indexing which can be the difference between 4h vs 22h. It also reduces the risk of an indexing failure after 22h due to e.g. lack of memory and uses much less peak memory overall.</li><li>Docker bind mounts (not volumes) are quite slow outside of Linux host environments (there are many other articles on this subject.)</li></ul><p>If you are looking for fast regexp or code search today, consider:</p><ul><li><a href=https://sourcegraph.com>Sourcegraph</a> (disclaimer: the author works here, but this article is not endorsed or affiliated in any way)</li><li><a href=https://github.com/google/zoekt>Zoekt</a></li><li><a href=https://github.com/BurntSushi/ripgrep>Ripgrep</a></li></ul><p>Follow this devlog for updates as we continue investigating faster ways to do regexp & ngram search at large scales.</p></div></main><script>function addAnchor(a){a.insertAdjacentHTML('afterbegin',`<a href="#${a.id}" class="hanchor" ariaLabel="Anchor">#</a> `)}document.addEventListener('DOMContentLoaded',function(){var a=document.querySelectorAll('h1[id], h2[id], h3[id], h4[id]');a&&a.forEach(addAnchor)})</script></div><div class=footer><div class=row-1><a href=https://hexops.com/privacy>Privacy matters</a>
<a href=https://github.com/sponsors/slimsag>Sponsor on GitHub</a>
<a href=https://machengine.org>machengine.org</a></div><div class=row-2><a href=/feed.xml><img alt="RSS feed" src="https://shields.io/badge/RSS-follow-green?logo=RSS"></a></div><div class=row-3><a href=https://hexops.com><img class="logo color-auto" alt="Hexops logo" src=https://raw.githubusercontent.com/hexops/media/234e15f265b19743c580a078b2d68660c92675d4/logo.svg height=50px></a></div></div></body></html>